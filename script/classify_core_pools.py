#!/usr/bin/env python3
"""
Script para classificar pools como core ou non-core baseado no hist√≥rico do CSV.

Regra l√≥gica:
Uma pool √© CORE em uma data D se existir no CSV hist√≥rico uma linha tal que:
- address = pool_address
- D >= added_date
- (removed_date IS NULL OR D < removed_date)

Caso contr√°rio, ela √© NON-CORE.

Requisitos:
1. Arquivo de hist√≥rico de core pools (core_pools_results.csv):
   - Deve conter colunas: 'address', 'added_date', 'removed_date' (opcional)
   - Cada linha representa um per√≠odo em que uma pool foi Core Pool
   - removed_date pode ser NULL se a pool ainda √© Core Pool

2. Dataset di√°rio (veBAL.csv por padr√£o):
   - Deve conter colunas com address da pool e data
   - O script detecta automaticamente colunas com nomes como:
     * Address: 'project_contract_address', 'address', 'pool_address', etc.
     * Date: 'block_date', 'date', 'day', 'timestamp', etc.

3. Arquivo de sa√≠da:
   - classification_core_pools.csv com colunas: 'address', 'day', 'is_core'
"""
import pandas as pd
from pathlib import Path
from datetime import datetime
import numpy as np

# Configura√ß√µes
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "data"

# Arquivos de entrada (ajuste conforme necess√°rio)
# Hist√≥rico de core pools: deve conter colunas 'address', 'added_date', 'removed_date' (opcional)
CORE_POOLS_HISTORY_FILE = DATA_DIR / "core_pools_results.csv"  # Hist√≥rico de core pools
# Dataset di√°rio: deve conter colunas com address da pool e data
DAILY_DATASET_FILE = DATA_DIR / "veBAL.csv"  # Dataset di√°rio (veBAL.csv)

# Arquivo de sa√≠da
OUTPUT_FILE = DATA_DIR / "classification_core_pools.csv"


def classify_core_pools(
    core_pools_file: Path = CORE_POOLS_HISTORY_FILE,
    daily_dataset_file: Path = DAILY_DATASET_FILE,
    output_file: Path = OUTPUT_FILE
):
    """
    Classifica pools como core ou non-core baseado no hist√≥rico.
    
    Args:
        core_pools_file: Caminho para o CSV hist√≥rico de core pools
        daily_dataset_file: Caminho para o dataset di√°rio
        output_file: Caminho para o arquivo de sa√≠da
    """
    print("=" * 60)
    print("üöÄ Classifica√ß√£o de Core Pools")
    print("=" * 60)
    
    print("\nüìñ Lendo arquivos...")
    
    # Verificar se os arquivos existem
    if not core_pools_file.exists():
        raise FileNotFoundError(f"Arquivo n√£o encontrado: {core_pools_file}")
    if not daily_dataset_file.exists():
        raise FileNotFoundError(f"Arquivo n√£o encontrado: {daily_dataset_file}")
    
    # Ler o CSV hist√≥rico de core pools
    core_pools_df = pd.read_csv(core_pools_file)
    
    # Ler o dataset di√°rio
    daily_df = pd.read_csv(daily_dataset_file)
    
    print(f"‚úÖ Core pools CSV: {len(core_pools_df):,} linhas")
    print(f"‚úÖ Dataset di√°rio: {len(daily_df):,} linhas")
    
    # Mostrar colunas dispon√≠veis para debug
    print(f"\nüìã Colunas no hist√≥rico de core pools: {list(core_pools_df.columns)}")
    print(f"üìã Colunas no dataset di√°rio: {list(daily_df.columns)}")
    
    # Verificar colunas necess√°rias no hist√≥rico de core pools
    required_core_cols = ['address', 'added_date']
    missing_core = [col for col in required_core_cols if col not in core_pools_df.columns]
    
    if missing_core:
        raise ValueError(f"Colunas faltando no hist√≥rico de core pools: {missing_core}")
    
    # Detectar automaticamente colunas do dataset di√°rio
    # Tentar diferentes nomes poss√≠veis para address e date
    address_col = None
    date_col = None
    
    # Poss√≠veis nomes para coluna de address
    address_candidates = ['project_contract_address', 'address', 'pool_address', 'pool_id', 'contract_address']
    for col in address_candidates:
        if col in daily_df.columns:
            address_col = col
            break
    
    # Poss√≠veis nomes para coluna de data
    date_candidates = ['block_date', 'date', 'day', 'timestamp', 'block_timestamp']
    for col in date_candidates:
        if col in daily_df.columns:
            date_col = col
            break
    
    if address_col is None:
        raise ValueError(
            f"N√£o foi poss√≠vel encontrar coluna de address no dataset di√°rio. "
            f"Colunas dispon√≠veis: {list(daily_df.columns)}. "
            f"Tente renomear uma coluna para um destes nomes: {address_candidates}"
        )
    
    if date_col is None:
        raise ValueError(
            f"N√£o foi poss√≠vel encontrar coluna de data no dataset di√°rio. "
            f"Colunas dispon√≠veis: {list(daily_df.columns)}. "
            f"Tente renomear uma coluna para um destes nomes: {date_candidates}"
        )
    
    print(f"\n‚úÖ Colunas detectadas:")
    print(f"   Address: {address_col}")
    print(f"   Date: {date_col}")
    
    # Converter datas para datetime
    print("\nüîÑ Convertendo datas...")
    core_pools_df['added_date'] = pd.to_datetime(core_pools_df['added_date'], errors='coerce')
    
    # Verificar se removed_date existe, se n√£o, criar coluna vazia
    if 'removed_date' not in core_pools_df.columns:
        core_pools_df['removed_date'] = pd.NaT
    else:
        core_pools_df['removed_date'] = pd.to_datetime(core_pools_df['removed_date'], errors='coerce')
    
    daily_df[date_col] = pd.to_datetime(daily_df[date_col], errors='coerce')
    
    # Remover timezone se existir (tornar tudo tz-naive)
    def remove_timezone(series):
        """Remove timezone de uma s√©rie de datetime se existir."""
        try:
            if hasattr(series.dt, 'tz') and series.dt.tz is not None:
                return series.dt.tz_localize(None)
        except (AttributeError, TypeError):
            pass
        return series
    
    core_pools_df['added_date'] = remove_timezone(core_pools_df['added_date'])
    core_pools_df['removed_date'] = remove_timezone(core_pools_df['removed_date'])
    daily_df[date_col] = remove_timezone(daily_df[date_col])
    
    # Remover linhas com datas inv√°lidas
    initial_daily_count = len(daily_df)
    daily_df = daily_df.dropna(subset=[date_col])
    core_pools_df = core_pools_df.dropna(subset=['added_date'])
    
    if len(daily_df) < initial_daily_count:
        print(f"‚ö†Ô∏è  Removidas {initial_daily_count - len(daily_df):,} linhas com datas inv√°lidas do dataset di√°rio")
    
    print(f"‚úÖ Dataset di√°rio ap√≥s limpeza: {len(daily_df):,} linhas")
    print(f"‚úÖ Hist√≥rico de core pools ap√≥s limpeza: {len(core_pools_df):,} linhas")
    
    print("\nüîç Classificando pools...")
    
    # M√©todo otimizado usando merge e condi√ß√µes vetorizadas
    # Criar uma c√≥pia do dataset di√°rio com colunas normalizadas
    result_df = daily_df[[address_col, date_col]].copy()
    result_df.columns = ['address', 'day']
    result_df = result_df.drop_duplicates()
    
    print(f"üìä Total de combina√ß√µes √∫nicas (address, day): {len(result_df):,}")
    
    # Fazer merge com o hist√≥rico de core pools
    # Isso cria todas as combina√ß√µes poss√≠veis
    merged = result_df.merge(
        core_pools_df[['address', 'added_date', 'removed_date']],
        on='address',
        how='left',
        suffixes=('', '_core')
    )
    
    print(f"üìä Total de combina√ß√µes ap√≥s merge: {len(merged):,}")
    
    # Aplicar a l√≥gica de classifica√ß√£o de forma vetorizada
    # Uma pool √© CORE se:
    # - day >= added_date
    # - (removed_date IS NULL OR day < removed_date)
    
    # Filtrar apenas registros onde day >= added_date
    valid_intervals = merged[
        (merged['day'] >= merged['added_date'])
    ].copy()
    
    # Para cada intervalo v√°lido, verificar se day < removed_date (ou removed_date √© NULL)
    valid_intervals['is_valid_interval'] = (
        valid_intervals['removed_date'].isna() | 
        (valid_intervals['day'] < valid_intervals['removed_date'])
    )
    
    # Filtrar apenas intervalos v√°lidos
    core_intervals = valid_intervals[valid_intervals['is_valid_interval']]
    
    # Agora, para cada (address, day) √∫nico, verificar se existe pelo menos um intervalo v√°lido
    core_classification = core_intervals.groupby(['address', 'day']).size().reset_index(name='count')
    core_classification['is_core'] = True
    
    # Fazer merge de volta com o dataset original
    result_df = result_df.merge(
        core_classification[['address', 'day', 'is_core']],
        on=['address', 'day'],
        how='left'
    )
    
    # Preencher False onde n√£o h√° classifica√ß√£o (n√£o √© core)
    result_df['is_core'] = result_df['is_core'].fillna(False).astype(bool)
    
    # Estat√≠sticas
    total_rows = len(result_df)
    core_rows = result_df['is_core'].sum()
    non_core_rows = total_rows - core_rows
    
    print(f"\nüìä Estat√≠sticas:")
    print(f"  Total de combina√ß√µes √∫nicas: {total_rows:,}")
    print(f"  Core: {core_rows:,} ({100 * core_rows / total_rows:.2f}%)")
    print(f"  Non-core: {non_core_rows:,} ({100 * non_core_rows / total_rows:.2f}%)")
    
    # Ordenar por address e day
    result_df = result_df.sort_values(['address', 'day'])
    
    # Salvar o resultado
    print(f"\nüíæ Salvando resultado em {output_file}...")
    result_df.to_csv(output_file, index=False)
    
    print(f"‚úÖ Arquivo salvo com sucesso!")
    print(f"   Total de linhas no arquivo de sa√≠da: {len(result_df):,}")
    
    # Mostrar algumas amostras
    print("\nüìã Amostra dos resultados (primeiras 20 linhas):")
    print(result_df.head(20).to_string(index=False))
    
    # Verificar alguns casos espec√≠ficos para valida√ß√£o
    print("\nüîç Valida√ß√£o - Verificando alguns casos espec√≠ficos:")
    
    # Pegar alguns addresses √∫nicos para verificar
    sample_addresses = result_df['address'].unique()[:3]
    
    for address in sample_addresses:
        address_data = result_df[result_df['address'] == address].head(10)
        if len(address_data) > 0:
            print(f"\n  Address: {address}")
            print(address_data.to_string(index=False))
    
    # Verificar transi√ß√µes core -> non-core -> core
    print("\nüîç Verificando transi√ß√µes Core <-> Non-Core:")
    transitions = []
    for address in result_df['address'].unique()[:5]:
        addr_data = result_df[result_df['address'] == address].sort_values('day')
        if len(addr_data) > 1:
            changes = addr_data['is_core'].ne(addr_data['is_core'].shift()).sum()
            if changes > 0:
                transitions.append({
                    'address': address,
                    'transitions': changes,
                    'first_date': addr_data['day'].min(),
                    'last_date': addr_data['day'].max()
                })
    
    if transitions:
        transitions_df = pd.DataFrame(transitions)
        print(transitions_df.to_string(index=False))
    
    return result_df


def main():
    """Fun√ß√£o principal"""
    try:
        result_df = classify_core_pools()
        print("\n" + "=" * 60)
        print("‚úÖ Processo conclu√≠do com sucesso!")
        print("=" * 60)
        return result_df
    except Exception as e:
        print(f"\n‚ùå Erro durante o processamento: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()
